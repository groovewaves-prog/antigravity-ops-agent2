import streamlit as st
import graphviz
import os
import time
import google.generativeai as genai
import json
import re
import pandas as pd
from google.api_core import exceptions as google_exceptions

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç¾¤ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from data import TOPOLOGY
from logic import CausalInferenceEngine, Alarm, simulate_cascade_failure
from network_ops import run_diagnostic_simulation, generate_remediation_commands, predict_initial_symptoms, generate_fake_log_by_ai
from verifier import verify_log_content, format_verification_report
from inference_engine import LogicalRCA

# --- ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="Antigravity Autonomous", page_icon="âš¡", layout="wide")

# ==========================================
# é–¢æ•°å®šç¾©
# ==========================================
def find_target_node_id(topology, node_type=None, layer=None, keyword=None):
    """ãƒˆãƒãƒ­ã‚¸ãƒ¼ã‹ã‚‰æ¡ä»¶ã«åˆã†ãƒãƒ¼ãƒ‰IDã‚’æ¤œç´¢"""
    for node_id, node in topology.items():
        if node_type and node.type != node_type: continue
        if layer and node.layer != layer: continue
        if keyword:
            hit = False
            if keyword in node_id: hit = True
            for v in node.metadata.values():
                if isinstance(v, str) and keyword in v: hit = True
            if not hit: continue
        return node_id
    return None

def load_config_by_id(device_id):
    """configsãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    possible_paths = [f"configs/{device_id}.txt", f"{device_id}.txt"]
    for path in possible_paths:
        if os.path.exists(path):
            try:
                with open(path, "r", encoding="utf-8") as f:
                    return f.read()
            except Exception:
                pass
    return "Config file not found."

def generate_content_with_retry(model, prompt, stream=True, retries=3):
    """503ã‚¨ãƒ©ãƒ¼å¯¾ç­–ã®ãƒªãƒˆãƒ©ã‚¤ä»˜ãç”Ÿæˆé–¢æ•°"""
    for i in range(retries):
        try:
            return model.generate_content(prompt, stream=stream)
        except google_exceptions.ServiceUnavailable:
            if i == retries - 1: raise
            time.sleep(2 * (i + 1))
    return None


def _pick_first(mapping: dict, keys: list[str], default: str = "") -> str:
    """Return the first non-empty value for the given keys from mapping (stringify scalars)."""
    for k in keys:
        try:
            v = mapping.get(k, None)
        except Exception:
            v = None
        if v is None:
            continue
        if isinstance(v, (int, float, bool)):
            s = str(v)
            if s:
                return s
        elif isinstance(v, str):
            if v.strip():
                return v.strip()
        else:
            # for non-string, try json
            try:
                s = json.dumps(v, ensure_ascii=False)
                if s and s != "null":
                    return s
            except Exception:
                continue
    return default


def _build_ci_context_for_chat(target_node_id: str) -> dict:
    """ãƒãƒ£ãƒƒãƒˆç”¨ã®CIã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æœ€å°é™ã§æ§‹ç¯‰ã—ã¾ã™ï¼ˆCI/Configã‚’ãƒ•ãƒ«æ´»ç”¨ã€ã‚­ãƒ¼æºã‚Œã‚’å¸åï¼‰ã€‚"""
    node = TOPOLOGY.get(target_node_id) if target_node_id else None
    md = (getattr(node, "metadata", None) or {}) if node else {}

    ci = {
        "device_id": target_node_id or "",
        "hostname": _pick_first(md, ["hostname", "host", "name"], default=(target_node_id or "")),
        "vendor": _pick_first(md, ["vendor", "manufacturer", "maker", "brand"], default=""),
        "os": _pick_first(md, ["os", "platform", "os_name", "software", "sw"], default=""),
        "model": _pick_first(md, ["model", "hw_model", "product", "sku"], default=""),
        "role": _pick_first(md, ["role", "type", "device_role"], default=""),
        "layer": _pick_first(md, ["layer", "level", "network_layer"], default=""),
        "site": _pick_first(md, ["site", "dc", "datacenter", "location"], default=""),
        "tenant": _pick_first(md, ["tenant", "customer", "org", "company"], default=""),
        "mgmt_ip": _pick_first(md, ["mgmt_ip", "management_ip", "management", "oob_ip"], default=""),
        "interfaces": md.get("interfaces", ""),
    }

    # Config ã¯é•·ã„ã®ã§æŠœç²‹ï¼ˆå­˜åœ¨ã™ã‚Œã°æœ€å¤§1500æ–‡å­—ï¼‰
    try:
        conf = load_config_by_id(target_node_id) if target_node_id else ""
        if conf:
            ci["config_excerpt"] = conf[:1500]
    except Exception:
        pass

    return ci


def _safe_chunk_text(chunk) -> str:
    """google.generativeai ã® stream chunk ã‹ã‚‰å®‰å…¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å–ã‚Šå‡ºã—ã¾ã™ã€‚"""
    # chunk.text ã¯ ValueError ã«ãªã‚Šå¾—ã‚‹
    try:
        t = getattr(chunk, "text", "")
        if t:
            return t
    except Exception:
        pass

    # candidates -> content -> parts ã‹ã‚‰æ‹¾ã†
    try:
        cands = getattr(chunk, "candidates", None) or []
        if not cands:
            return ""
        content = getattr(cands[0], "content", None)
        parts = getattr(content, "parts", None) or []
        out = []
        for p in parts:
            tx = getattr(p, "text", "")
            if tx:
                out.append(tx)
        return "".join(out)
    except Exception:
        return ""




def run_diagnostic_simulation_no_llm(selected_scenario, target_node_obj):
    """LLMã‚’å‘¼ã°ãªã„ç–‘ä¼¼è¨ºæ–­ï¼ˆ503/ã‚³ã‚¹ãƒˆå¯¾ç­–ï¼‰ã€‚UXã¯ç¶­æŒã—ã¤ã¤ã€ææ–™ã‚’å¢—ã‚„ã™ãŸã‚ã®ãƒ­ã‚°ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    é‡è¦: ã€Œä¿®å¾©å®Ÿè¡Œ(Execute)ã€ã§å¾©æ—§æˆåŠŸã—ãŸå¾Œã¯ã€åŒä¸€ã‚·ãƒŠãƒªã‚ªã«é™ã‚ŠæˆåŠŸå´ã®ç–‘ä¼¼ãƒ­ã‚°ã‚’è¿”ã—ã¾ã™ã€‚
    """
    device_id = getattr(target_node_obj, "id", "UNKNOWN") if target_node_obj else "UNKNOWN"
    ts = time.strftime("%Y-%m-%d %H:%M:%S")
    lines = [
        f"[PROBE] ts={ts}",
        f"[PROBE] scenario={selected_scenario}",
        f"[PROBE] target_device={device_id}",
        "",
    ]

    # å¾©æ—§æˆåŠŸãƒ•ãƒ©ã‚°ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
    recovered_devices = st.session_state.get("recovered_devices") or {}
    recovered_map = st.session_state.get("recovered_scenario_map") or {}

    if recovered_devices.get(device_id) and recovered_map.get(device_id) == selected_scenario:
        # â€œå¾©æ—§å¾Œâ€ã®ç–‘ä¼¼ãƒ­ã‚°ï¼ˆæˆåŠŸï¼‰
        if "FW" in selected_scenario:
            lines += [
                "show chassis cluster status",
                "Redundancy group 0: healthy",
                "control link: up",
                "fabric link: up",
            ]
        elif "WAN" in selected_scenario or "WANå…¨å›ç·šæ–­" in selected_scenario:
            lines += [
                "show ip interface brief",
                "GigabitEthernet0/0 up up",
                "show ip bgp summary",
                "Neighbor 203.0.113.2 Established",
                "ping 203.0.113.2 repeat 5",
                "Success rate is 100 percent (5/5)",
            ]
        elif "L2SW" in selected_scenario:
            lines += [
                "show environment",
                "Fan: OK",
                "Temperature: OK",
                "show interface status",
                "Uplink: up",
            ]
        else:
            lines += [
                "show system alarms",
                "No active alarms",
                "ping 8.8.8.8 repeat 5",
                "Success rate is 100 percent (5/5)",
            ]

        return {
            "status": "SUCCESS",
            "sanitized_log": "\n".join(lines),
            "verification_log": "N/A",
            "device_id": device_id,
        }

    # â€œéšœå®³ä¸­â€ã®ç–‘ä¼¼ãƒ­ã‚°ï¼ˆç¾çŠ¶ç¶­æŒï¼‰
    if "WANå…¨å›ç·šæ–­" in selected_scenario or "[WAN]" in selected_scenario:
        lines += [
            "show ip interface brief",
            "GigabitEthernet0/0 down down",
            "show ip bgp summary",
            "Neighbor 203.0.113.2 Idle",
            "ping 203.0.113.2 repeat 5",
            "Success rate is 0 percent (0/5)",
        ]
    elif "FWç‰‡ç³»éšœå®³" in selected_scenario or "[FW]" in selected_scenario:
        lines += [
            "show chassis cluster status",
            "Redundancy group 0: degraded",
            "control link: down",
            "fabric link: up",
        ]
    elif "L2SW" in selected_scenario:
        lines += [
            "show environment",
            "Fan: FAIL",
            "Temperature: HIGH",
            "show interface status",
            "Uplink: flapping",
        ]
    else:
        lines += [
            "show system alarms",
            "No active alarms",
        ]

    return {
        "status": "SUCCESS",
        "sanitized_log": "\n".join(lines),
        "verification_log": "N/A",
        "device_id": device_id,
    }


def _hash_text(text: str) -> str:
    import hashlib
    return hashlib.sha256((text or "").encode("utf-8")).hexdigest()[:16]

def _extract_first_codeblock_after_heading(markdown_text: str, heading_keyword: str) -> str:
    """Extract the first fenced code block (``` ... ```) that appears *after* a heading containing heading_keyword.
    - Returns code content without fences.
    - If not found, returns empty string.
    This is intentionally simple and robust to avoid complex parsing / IF sprawl.
    """
    if not markdown_text or not heading_keyword:
        return ""
    # Find the heading position (supports '#', '##', etc. and also plain text headings)
    idx = markdown_text.find(heading_keyword)
    if idx < 0:
        return ""
    tail = markdown_text[idx:]
    # Find first fenced code block after the heading
    m = re.search(r"```[a-zA-Z0-9_+-]*\s*\n(.*?)\n```", tail, flags=re.DOTALL)
    if not m:
        return ""
    return (m.group(1) or "").strip()
def render_topology(alarms, root_cause_candidates):
    """ãƒˆãƒãƒ­ã‚¸ãƒ¼å›³ã®æç”» (AIåˆ¤å®šçµæœã‚’åæ˜ )"""
    graph = graphviz.Digraph()
    graph.attr(rankdir='TB')
    graph.attr('node', shape='box', style='rounded,filled', fontname='Helvetica')
    
    alarm_map = {a.device_id: a for a in alarms}
    alarmed_ids = set(alarm_map.keys())
    
    root_cause_ids = {c['id'] for c in root_cause_candidates if c['prob'] > 0.6}
    
    # AIåˆ¤å®šçµæœã®ãƒãƒƒãƒ”ãƒ³ã‚°
    node_status_map = {c['id']: c['type'] for c in root_cause_candidates}
    
    for node_id, node in TOPOLOGY.items():
        color = "#e8f5e9"
        penwidth = "1"
        fontcolor = "black"
        label = f"{node_id}\n({node.type})"
        
        red_type = node.metadata.get("redundancy_type")
        if red_type: label += f"\n[{red_type} Redundancy]"
        vendor = node.metadata.get("vendor")
        if vendor: label += f"\n[{vendor}]"

        status_type = node_status_map.get(node_id, "Normal")
        
        if "Hardware/Physical" in status_type or "Critical" in status_type or "Silent" in status_type:
            color = "#ffcdd2" 
            penwidth = "3"
            label += "\n[ROOT CAUSE]"
        elif "Network/Unreachable" in status_type or "Network/Secondary" in status_type:
            color = "#cfd8dc" 
            fontcolor = "#546e7a"
            label += "\n[Unreachable]"
        elif node_id in alarmed_ids:
            color = "#fff9c4" 
        
        graph.node(node_id, label=label, fillcolor=color, color='black', penwidth=penwidth, fontcolor=fontcolor)
    
    for node_id, node in TOPOLOGY.items():
        if node.parent_id:
            graph.edge(node.parent_id, node_id)
            parent_node = TOPOLOGY.get(node.parent_id)
            if parent_node and parent_node.redundancy_group:
                partners = [n.id for n in TOPOLOGY.values() 
                           if n.redundancy_group == parent_node.redundancy_group and n.id != parent_node.id]
                for partner_id in partners:
                    graph.edge(partner_id, node_id)
    return graph

# --- UIæ§‹ç¯‰ ---
st.title("âš¡ Antigravity Autonomous Agent")

api_key = None
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
else:
    api_key = os.environ.get("GOOGLE_API_KEY")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.header("âš¡ Scenario Controller")
    SCENARIO_MAP = {
        "åŸºæœ¬ãƒ»åºƒåŸŸéšœå®³": ["æ­£å¸¸ç¨¼åƒ", "1. WANå…¨å›ç·šæ–­", "2. FWç‰‡ç³»éšœå®³", "3. L2SWã‚µã‚¤ãƒ¬ãƒ³ãƒˆéšœå®³"],
        "WAN Router": ["4. [WAN] é›»æºéšœå®³ï¼šç‰‡ç³»", "5. [WAN] é›»æºéšœå®³ï¼šä¸¡ç³»", "6. [WAN] BGPãƒ«ãƒ¼ãƒˆãƒ•ãƒ©ãƒƒãƒ”ãƒ³ã‚°", "7. [WAN] FANæ•…éšœ", "8. [WAN] ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯"],
        "Firewall (Juniper)": ["9. [FW] é›»æºéšœå®³ï¼šç‰‡ç³»", "10. [FW] é›»æºéšœå®³ï¼šä¸¡ç³»", "11. [FW] FANæ•…éšœ", "12. [FW] ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯"],
        "L2 Switch": ["13. [L2SW] é›»æºéšœå®³ï¼šç‰‡ç³»", "14. [L2SW] é›»æºéšœå®³ï¼šä¸¡ç³»", "15. [L2SW] FANæ•…éšœ", "16. [L2SW] ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯"],
        "è¤‡åˆãƒ»ãã®ä»–": ["17. [WAN] è¤‡åˆéšœå®³ï¼šé›»æºï¼†FAN", "18. [Complex] åŒæ™‚å¤šç™ºï¼šFW & AP", "99. [Live] Ciscoå®Ÿæ©Ÿè¨ºæ–­"]
    }
    selected_category = st.selectbox("å¯¾è±¡ã‚«ãƒ†ã‚´ãƒª:", list(SCENARIO_MAP.keys()))
    selected_scenario = st.radio("ç™ºç”Ÿã‚·ãƒŠãƒªã‚ª:", SCENARIO_MAP[selected_category])
    st.markdown("---")
    if api_key: st.success("API Connected")
    else:
        st.warning("API Key Missing")
        user_key = st.text_input("Google API Key", type="password")
        if user_key: api_key = user_key

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç† ---
if "current_scenario" not in st.session_state:
    st.session_state.current_scenario = "æ­£å¸¸ç¨¼åƒ"

# å¤‰æ•°åˆæœŸåŒ–
for key in ["live_result", "messages", "chat_session", "trigger_analysis", "verification_result", "generated_report", "verification_log", "last_report_cand_id", "logic_engine", "recovered_devices", "recovered_scenario_map"]:
    if key not in st.session_state:
        st.session_state[key] = None if key != "messages" and key != "trigger_analysis" else ([] if key == "messages" else False)


# å¾©æ—§çŠ¶æ…‹ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
if "recovered_devices" not in st.session_state:
    st.session_state.recovered_devices = {}
if "recovered_scenario_map" not in st.session_state:
    st.session_state.recovered_scenario_map = {}

# ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
if not st.session_state.logic_engine:
    st.session_state.logic_engine = LogicalRCA(TOPOLOGY)

# ã‚·ãƒŠãƒªã‚ªåˆ‡ã‚Šæ›¿ãˆæ™‚ã®ãƒªã‚»ãƒƒãƒˆ
if st.session_state.current_scenario != selected_scenario:
    st.session_state.current_scenario = selected_scenario
    # ã‚·ãƒŠãƒªã‚ªå¤‰æ›´æ™‚ã¯å¾©æ—§ãƒ•ãƒ©ã‚°ã‚‚ã‚¯ãƒªã‚¢ï¼ˆæœªä¿®å¾©ãªã®ã«OKã«ãªã‚‹ã®ã‚’é˜²ãï¼‰
    st.session_state.recovered_devices = {}
    st.session_state.recovered_scenario_map = {}
    st.session_state.messages = []      
    st.session_state.chat_session = None 
    st.session_state.live_result = None 
    st.session_state.trigger_analysis = False
    st.session_state.verification_result = None
    st.session_state.generated_report = None
    st.session_state.verification_log = None 
    st.session_state.last_report_cand_id = None
    if "remediation_plan" in st.session_state: del st.session_state.remediation_plan
    st.rerun()

# ==========================================
# ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯
# ==========================================
alarms = []
target_device_id = None
root_severity = "CRITICAL"
is_live_mode = False

# 1. ã‚¢ãƒ©ãƒ¼ãƒ ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
if "Live" in selected_scenario: is_live_mode = True
elif "WANå…¨å›ç·šæ–­" in selected_scenario:
    target_device_id = find_target_node_id(TOPOLOGY, node_type="ROUTER")
    if target_device_id: alarms = simulate_cascade_failure(target_device_id, TOPOLOGY)
elif "FWç‰‡ç³»éšœå®³" in selected_scenario:
    target_device_id = find_target_node_id(TOPOLOGY, node_type="FIREWALL")
    if target_device_id:
        alarms = [Alarm(target_device_id, "Heartbeat Loss", "WARNING")]
        root_severity = "WARNING"

elif "L2SWã‚µã‚¤ãƒ¬ãƒ³ãƒˆéšœå®³" in selected_scenario:
    target_device_id = "L2_SW_01"
    if target_device_id not in TOPOLOGY:
        target_device_id = find_target_node_id(TOPOLOGY, keyword="L2_SW")
    if target_device_id and target_device_id in TOPOLOGY:
        child_nodes = [nid for nid, n in TOPOLOGY.items() if n.parent_id == target_device_id]
        alarms = [Alarm(child, "Connection Lost", "CRITICAL") for child in child_nodes]
    else:
        st.error("Error: L2 Switch definition not found")

elif "è¤‡åˆéšœå®³" in selected_scenario:
    target_device_id = find_target_node_id(TOPOLOGY, node_type="ROUTER")
    if target_device_id:
        alarms = [
            Alarm(target_device_id, "Power Supply 1 Failed", "CRITICAL"),
            Alarm(target_device_id, "Fan Fail", "WARNING")
        ]
elif "åŒæ™‚å¤šç™º" in selected_scenario:
    fw_node = find_target_node_id(TOPOLOGY, node_type="FIREWALL")
    ap_node = find_target_node_id(TOPOLOGY, node_type="ACCESS_POINT")
    alarms = []
    if fw_node: alarms.append(Alarm(fw_node, "Heartbeat Loss", "WARNING"))
    if ap_node: alarms.append(Alarm(ap_node, "Connection Lost", "CRITICAL"))
    target_device_id = fw_node 
else:
    if "[WAN]" in selected_scenario: target_device_id = find_target_node_id(TOPOLOGY, node_type="ROUTER")
    elif "[FW]" in selected_scenario: target_device_id = find_target_node_id(TOPOLOGY, node_type="FIREWALL")
    elif "[L2SW]" in selected_scenario: target_device_id = find_target_node_id(TOPOLOGY, node_type="SWITCH", layer=4)

    if target_device_id:
        if "é›»æºéšœå®³ï¼šç‰‡ç³»" in selected_scenario:
            alarms = [Alarm(target_device_id, "Power Supply 1 Failed", "WARNING")]
            root_severity = "WARNING"
        elif "é›»æºéšœå®³ï¼šä¸¡ç³»" in selected_scenario:
            if "FW" in target_device_id:
                alarms = [Alarm(target_device_id, "Power Supply: Dual Loss (Device Down)", "CRITICAL")]
            else:
                alarms = simulate_cascade_failure(target_device_id, TOPOLOGY, "Power Supply: Dual Loss (Device Down)")
        elif "BGP" in selected_scenario:
            alarms = [Alarm(target_device_id, "BGP Flapping", "WARNING")]
            root_severity = "WARNING"
        elif "FAN" in selected_scenario:
            alarms = [Alarm(target_device_id, "Fan Fail", "WARNING")]
            root_severity = "WARNING"
        elif "ãƒ¡ãƒ¢ãƒª" in selected_scenario:
            alarms = [Alarm(target_device_id, "Memory High", "WARNING")]
            root_severity = "WARNING"

# 2. æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã«ã‚ˆã‚‹åˆ†æ
analysis_results = st.session_state.logic_engine.analyze(alarms)

# 3. ã‚³ãƒƒã‚¯ãƒ”ãƒƒãƒˆè¡¨ç¤º
selected_incident_candidate = None

st.markdown("### ğŸ›¡ï¸ AIOps ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ»ã‚³ãƒƒã‚¯ãƒ”ãƒƒãƒˆ")
col1, col2, col3 = st.columns(3)
with col1: st.metric("ğŸ“‰ ãƒã‚¤ã‚ºå‰Šæ¸›ç‡", "98.5%", "é«˜åŠ¹ç‡ç¨¼åƒä¸­")
with col2: st.metric("ğŸ“¨ å‡¦ç†ã‚¢ãƒ©ãƒ¼ãƒ æ•°", f"{len(alarms) * 15 if alarms else 0}ä»¶", "æŠ‘åˆ¶æ¸ˆ")
with col3: st.metric("ğŸš¨ è¦å¯¾å¿œã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ", f"{len([c for c in analysis_results if c['prob'] > 0.6])}ä»¶", "å¯¾å‡¦ãŒå¿…è¦")
st.markdown("---")

df_data = []
# â˜…ä¿®æ­£: ã‚¹ãƒ©ã‚¤ã‚¹åˆ¶é™ã‚’æ’¤å»ƒ (å…¨ä»¶è¡¨ç¤º)
# éšå±¤ãƒ­ã‚¸ãƒƒã‚¯ã«ã‚ˆã‚Šã€é‡è¦ãªã‚‚ã®(Tieré«˜)ãŒå…ˆé ­ã«æ¥ã‚‹ãŸã‚ã€å¤§é‡ã«ã‚ã£ã¦ã‚‚å•é¡Œãªã„
for rank, cand in enumerate(analysis_results, 1):
    status = "âšª ç›£è¦–ä¸­"
    action = "ğŸ‘ï¸ é™è¦³"
    
    if cand['prob'] > 0.8:
        status = "ğŸ”´ å±é™º (æ ¹æœ¬åŸå› )"
        action = "ğŸš€ è‡ªå‹•ä¿®å¾©ãŒå¯èƒ½"
    elif cand['prob'] > 0.6:
        status = "ğŸŸ¡ è­¦å‘Š (è¢«ç–‘ç®‡æ‰€)"
        action = "ğŸ” è©³ç´°èª¿æŸ»ã‚’æ¨å¥¨"
    
    if "Network/Unreachable" in cand['type'] or "Network/Secondary" in cand['type']:
        status = "âš« å¿œç­”ãªã— (ä¸Šä½éšœå®³)"
        action = "â›” å¯¾å¿œä¸è¦ (ä¸Šä½å¾©æ—§å¾…ã¡)"

    candidate_text = f"ãƒ‡ãƒã‚¤ã‚¹: {cand['id']} / åŸå› : {cand['label']}"
    if cand.get('verification_log'):
        candidate_text += " [ğŸ” Active Probe: å¿œç­”ãªã—]"
    
    # ãƒ‡ãƒãƒƒã‚°ç”¨ã«Tierã‚’è¡¨ç¤ºï¼ˆæœ¬ç•ªã§ã¯æ¶ˆã—ã¦ã‚‚è‰¯ã„ï¼‰
    # candidate_text += f" (Tier: {cand.get('tier')})"

    df_data.append({
        "é †ä½": rank,
        "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": status,
        "æ ¹æœ¬åŸå› å€™è£œ": candidate_text,
        "ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢": cand['prob'],
        "æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³": action,
        "ID": cand['id'],
        "Type": cand['type']
    })

df = pd.DataFrame(df_data)
st.info("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã®è¡Œã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€å³å´ã«è©³ç´°åˆ†æã¨å¾©æ—§ãƒ—ãƒ©ãƒ³ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

event = st.dataframe(
    df,
    column_order=["é †ä½", "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", "æ ¹æœ¬åŸå› å€™è£œ", "ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢", "æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"],
    column_config={
        "ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢": st.column_config.ProgressColumn("ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ (0-1.0)", format="%.2f", min_value=0, max_value=1),
    },
    use_container_width=True,
    hide_index=True,
    selection_mode="single-row",
    on_select="rerun"
)

if len(event.selection.rows) > 0:
    idx = event.selection.rows[0]
    sel_row = df.iloc[idx]
    for res in analysis_results:
        if res['id'] == sel_row['ID'] and res['type'] == sel_row['Type']:
            selected_incident_candidate = res
            break
else:
    selected_incident_candidate = analysis_results[0] if analysis_results else None


# 4. ç”»é¢åˆ†å‰²
col_map, col_chat = st.columns([1.2, 1])

# === å·¦ã‚«ãƒ©ãƒ : ãƒˆãƒãƒ­ã‚¸ãƒ¼ã¨è¨ºæ–­ ===
with col_map:
    st.subheader("ğŸŒ Network Topology")
    
    current_root_node = None
    current_severity = "WARNING"
    
    if selected_incident_candidate and selected_incident_candidate["prob"] > 0.6:
        current_root_node = TOPOLOGY.get(selected_incident_candidate["id"])
        if "Hardware/Physical" in selected_incident_candidate["type"] or "Critical" in selected_incident_candidate["type"] or "Silent" in selected_incident_candidate["type"]:
            current_severity = "CRITICAL"
        else:
            current_severity = "WARNING"

    elif target_device_id:
        current_root_node = TOPOLOGY.get(target_device_id)
        current_severity = root_severity

    st.graphviz_chart(render_topology(alarms, analysis_results), use_container_width=True)

    st.markdown("---")
    st.subheader("ğŸ› ï¸ Auto-Diagnostics")
    
    if st.button("ğŸš€ è¨ºæ–­å®Ÿè¡Œ (Run Diagnostics)", type="primary"):
        if not api_key:
            st.error("API Key Required")
        else:
            with st.status("Agent Operating...", expanded=True) as status:
                st.write("ğŸ”Œ Connecting to device...")
                target_node_obj = TOPOLOGY.get(target_device_id) if target_device_id else None
                is_live_mode = bool(st.session_state.get('api_connected')) and ('[Live]' in selected_scenario or 'Live' in selected_scenario)
                
                res = run_diagnostic_simulation(selected_scenario, target_node_obj, api_key) if is_live_mode else run_diagnostic_simulation_no_llm(selected_scenario, target_node_obj)
                st.session_state.live_result = res
                
                if res["status"] == "SUCCESS":
                    st.write("âœ… Log Acquired & Sanitized.")
                    status.update(label="Diagnostics Complete!", state="complete", expanded=False)
                    log_content = res.get('sanitized_log', "")
                    verification = verify_log_content(log_content)
                    st.session_state.verification_result = verification
                    st.session_state.trigger_analysis = True
                elif res["status"] == "SKIPPED":
                    status.update(label="No Action Required", state="complete")
                else:
                    st.write("âŒ Connection Failed.")
                    status.update(label="Diagnostics Failed", state="error")
            st.rerun()

    if st.session_state.live_result:
        res = st.session_state.live_result
        if res["status"] == "SUCCESS":
            st.markdown("#### ğŸ“„ Diagnostic Results")
            with st.container(border=True):
                if selected_incident_candidate and selected_incident_candidate.get("verification_log"):
                    st.caption("ğŸ¤– Active Probe / Verification Log")
                    st.code(selected_incident_candidate["verification_log"], language="text")
                    st.divider()

                if st.session_state.verification_result:
                    v = st.session_state.verification_result
                    c1, c2, c3 = st.columns(3)
                    c1.metric("Ping Status", v.get('ping_status'))
                    c2.metric("Interface", v.get('interface_status'))
                    c3.metric("Hardware", v.get('hardware_status'))
                
                st.divider()
                st.caption("ğŸ”’ Raw Logs (Sanitized)")
                st.code(res["sanitized_log"], language="text")
        elif res["status"] == "ERROR":
            st.error(f"è¨ºæ–­ã‚¨ãƒ©ãƒ¼: {res.get('error')}")

# === å³ã‚«ãƒ©ãƒ : åˆ†æãƒ¬ãƒãƒ¼ãƒˆ ===
with col_chat:
    st.subheader("ğŸ“ AI Analyst Report")
    
    if selected_incident_candidate:
        cand = selected_incident_candidate
        
        # --- A. çŠ¶æ³å ±å‘Š (Situation Report) ---
        if "generated_report" not in st.session_state or st.session_state.generated_report is None:
            st.info(f"ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆé¸æŠä¸­: **{cand['id']}** ({cand['label']})")
            
            if api_key and selected_scenario != "æ­£å¸¸ç¨¼åƒ":
                if st.button("ğŸ“ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ (Generate Report)"):
                    
                    report_container = st.empty()
                    target_conf = load_config_by_id(cand['id'])
                    
                    genai.configure(api_key=api_key)
                    model = genai.GenerativeModel("gemma-3-12b-it")

                    verification_context = cand.get("verification_log", "ç‰¹ã«ãªã—")
                    target_conf = load_config_by_id(cand['id'])

                    # CI/ãƒˆãƒãƒ­ã‚¸ãƒ¼æƒ…å ±
                    t_node = TOPOLOGY.get(cand["id"])
                    t_node_dict = {
    "id": getattr(t_node, "id", None),
    "type": getattr(t_node, "type", None),
    "layer": getattr(t_node, "layer", None),
    "metadata": getattr(t_node, "metadata", {}) or {},
    "parent": getattr(t_node, "parent", None),
    "children": getattr(t_node, "children", []) or [],
} if t_node else {}

                    parent_id = t_node.parent_id if t_node else None
                    children_ids = [
                        nid for nid, n in TOPOLOGY.items()
                        if getattr(n, "parent_id", None) == cand["id"]
                    ]
                    topology_context = {"node": t_node_dict, "parent_id": parent_id, "children_ids": children_ids}

                    cache_key = "|".join([
                        selected_scenario,
                        str(cand.get("id")),
                        _hash_text(json.dumps(topology_context, ensure_ascii=False, sort_keys=True)),
                        _hash_text(target_conf or ""),
                        _hash_text(verification_context or ""),
                    ])

                    if "report_cache" not in st.session_state:
                        st.session_state.report_cache = {}

                    if cache_key in st.session_state.report_cache:
                        full_text = st.session_state.report_cache[cache_key]
                        report_container.markdown(full_text)
                    else:
                        prompt = f"""
ã‚ãªãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é‹ç”¨è€…å‘ã‘ã®AIåˆ†æå®˜ã§ã™ã€‚ä»¥ä¸‹ã®äº‹å®Ÿï¼ˆCIæƒ…å ±/ãƒˆãƒãƒ­ã‚¸ãƒ¼/config/ãƒ­ã‚°ï¼‰ã‹ã‚‰ã€é‹ç”¨è€…ãŒä½œæ¥­ã«ä½¿ãˆã‚‹çŠ¶æ³å ±å‘Šã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

æ–‡ä½“:
- å¿…ãšã€Œã§ã™/ã¾ã™èª¿ã€ã§çµ±ä¸€ã—ã¦ãã ã•ã„ã€‚

ç¦æ­¢:
- ã€Œç¾åœ¨ã€åŸå› ç©¶æ˜ã¨å¾©æ—§ä½œæ¥­ã‚’æœ€å„ªå…ˆã§é€²ã‚ã¦ãŠã‚Šã¾ã™ã€
- ã€Œé€²æ—çŠ¶æ³ã¯éšæ™‚ã€ã”å ±å‘Šã„ãŸã—ã¾ã™ã€
- ã€Œæ¤œè¨ã‚’åŠ é€Ÿã•ã›ã¾ã™ã€
ãªã©ã®å¯¾å¤–å‘ã‘å®šå‹å¥ã¯æ›¸ã‹ãªã„ã§ãã ã•ã„ã€‚

ä¸æ˜ç‚¹:
- ä¸æ˜ãªç‚¹ã¯ã€Œæœªç¢ºèªã€ã¨ã—ã€æ¨æ¸¬ã¯ã€Œæ¨å®šã€ã¨æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚

å‡ºåŠ›:
- Markdown
- æ¬¡ã®ç« ç«‹ã¦ã‚’å¿…ãšå«ã‚ã¦ãã ã•ã„ï¼ˆè¦‹å‡ºã—æ–‡è¨€ã‚’å¤‰æ›´ã—ãªã„ï¼‰:
1. éšœå®³æ¦‚è¦
2. å½±éŸ¿
3. è©³ç´°æƒ…å ±
4. å¯¾å¿œã¨ç‰¹å®šæ ¹æ‹ 
5. ä»Šå¾Œã®å¯¾å¿œ
6. å¾©æ—§ã‚³ãƒãƒ³ãƒ‰ï¼ˆå®Ÿæ–½å‰æãƒ»æ³¨æ„ç‚¹ï¼‰
7. æ­£å¸¸æ€§ç¢ºèªã‚³ãƒãƒ³ãƒ‰ï¼ˆãƒ¬ãƒãƒ¼ãƒˆç”¨ï¼‰

å…¥åŠ›:
- ã‚·ãƒŠãƒªã‚ª: {selected_scenario}
- å¯¾è±¡æ©Ÿå™¨ID: {cand['id']}
- CI/ãƒˆãƒãƒ­ã‚¸ãƒ¼: {json.dumps(topology_context, ensure_ascii=False)}
- Config(æŠœç²‹): {(target_conf or 'ãªã—')[:2000]}
- æ¤œè¨¼ãƒ­ã‚°: {verification_context}

ã‚³ãƒãƒ³ãƒ‰ã¯å¿…ãš ``` ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã§å›²ã‚“ã§ãã ã•ã„ã€‚
"""

                        try:
                            response = generate_content_with_retry(model, prompt, stream=False)
                            full_text = response.text if hasattr(response, "text") and response.text else str(response)
                            if not full_text:
                                full_text = "ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆç©ºã®å¿œç­”ï¼‰ã€‚"
                            report_container.markdown(full_text)
                            st.session_state.report_cache[cache_key] = full_text
                        except google_exceptions.ServiceUnavailable:
                            full_text = "âš ï¸ ç¾åœ¨ã€AIãƒ¢ãƒ‡ãƒ«ãŒæ··é›‘ã—ã¦ã„ã¾ã™ (503 Error)ã€‚æ™‚é–“ã‚’ç½®ã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
                            report_container.markdown(full_text)
                        except Exception as e:
                            full_text = f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {type(e).__name__}: {e}"
                            report_container.markdown(full_text)

                    st.session_state.generated_report = full_text
        else:
            st.markdown(st.session_state.generated_report)
            if st.button("ğŸ”„ ãƒ¬ãƒãƒ¼ãƒˆå†ä½œæˆ"):
                st.session_state.generated_report = None
                st.rerun()

    # --- B. è‡ªå‹•ä¿®å¾© & ãƒãƒ£ãƒƒãƒˆ ---
    st.markdown("---")
    st.subheader("ğŸ¤– Remediation & Chat")

    if selected_incident_candidate and selected_incident_candidate["prob"] > 0.6:
        st.markdown(f"""
        <div style="background-color:#e8f5e9;padding:10px;border-radius:5px;border:1px solid #4caf50;color:#2e7d32;margin-bottom:10px;">
            <strong>âœ… AI Analysis Completed</strong><br>
            ç‰¹å®šã•ã‚ŒãŸåŸå›  <b>{selected_incident_candidate['id']}</b> ã«å¯¾ã™ã‚‹å¾©æ—§æ‰‹é †ãŒåˆ©ç”¨å¯èƒ½ã§ã™ã€‚<br>
            (ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢: <span style="font-size:1.2em;font-weight:bold;">{selected_incident_candidate['prob']*100:.0f}</span>)
        </div>
        """, unsafe_allow_html=True)

        if "remediation_plan" not in st.session_state:
            if st.button("âœ¨ ä¿®å¾©ãƒ—ãƒ©ãƒ³ã‚’ä½œæˆ (Generate Fix)"):
                 if "generated_report" not in st.session_state or not st.session_state.generated_report:
                     st.warning("å…ˆã«ã€ŒğŸ“ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ (Generate Report)ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                 else:
                     plan_md = st.session_state.generated_report
                     st.session_state.recovery_commands = _extract_first_codeblock_after_heading(plan_md, "å¾©æ—§ã‚³ãƒãƒ³ãƒ‰")
                     st.session_state.verification_commands = _extract_first_codeblock_after_heading(plan_md, "æ­£å¸¸æ€§ç¢ºèª")
                     st.session_state.remediation_plan = plan_md
                     st.rerun()
        
        if "remediation_plan" in st.session_state:
            with st.container(border=True):
                st.info("AI Generated Recovery Procedure")
                st.markdown(st.session_state.remediation_plan)
            
            col_exec1, col_exec2 = st.columns(2)
            
            with col_exec1:
                if st.button("ğŸš€ ä¿®å¾©å®Ÿè¡Œ (Execute)", type="primary"):
                    if not api_key:
                        st.error("API Key Required")
                    else:
                        with st.status("Autonomic Remediation in progress...", expanded=True) as status:
                            st.write("âš™ï¸ Applying Configuration...")
                            time.sleep(1.5) 
                            
                            st.write("ğŸ” Running Verification Commands...")
                            target_node_obj = TOPOLOGY.get(selected_incident_candidate["id"])
                            verification_log = generate_fake_log_by_ai("æ­£å¸¸ç¨¼åƒ", target_node_obj, api_key)
                            st.session_state.verification_log = verification_log
                            
                            st.write("âœ… Verification Completed.")
                            status.update(label="Process Finished", state="complete", expanded=False)
                        
                        st.success("Remediation Process Finished.")

            with col_exec2:
                 if st.button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«"):
                    del st.session_state.remediation_plan
                    st.session_state.verification_log = None
                    st.rerun()
            
            if st.session_state.get("verification_log"):
                st.markdown("#### ğŸ” Post-Fix Verification Logs")
                st.code(st.session_state.verification_log, language="text")
                
                is_success = "up" in st.session_state.verification_log.lower() or "ok" in st.session_state.verification_log.lower()
                
                if is_success:
                    # å¾©æ—§æˆåŠŸãƒ•ãƒ©ã‚°ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰ã€‚æ¬¡å›ã®ã€Œè¨ºæ–­å®Ÿè¡Œã€ã§æˆåŠŸå´ã®ç–‘ä¼¼ãƒ­ã‚°ã‚’è¿”ã—ã¾ã™ã€‚
                    st.session_state.recovered_devices = st.session_state.get("recovered_devices") or {}
                    st.session_state.recovered_scenario_map = st.session_state.get("recovered_scenario_map") or {}
                    st.session_state.recovered_devices[target_device_id] = True
                    st.session_state.recovered_scenario_map[target_device_id] = selected_scenario
                    st.balloons()
                    st.success("âœ… System Recovered Successfully!")
                else:
                    st.warning("âš ï¸ Verification indicates potential issues. Please check manually.")

                if st.button("ãƒ‡ãƒ¢ã‚’çµ‚äº†ã—ã¦ãƒªã‚»ãƒƒãƒˆ"):
                    del st.session_state.remediation_plan
                    st.session_state.verification_log = None
                    st.session_state.current_scenario = "æ­£å¸¸ç¨¼åƒ"
                    st.rerun()
    else:
        if selected_incident_candidate:
            score = selected_incident_candidate['prob'] * 100
            st.warning(f"""
            âš ï¸ **è‡ªå‹•ä¿®å¾©ã¯ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã¾ã™**
            ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã®ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã¯ **{score:.0f}** ã§ã™ã€‚
            èª¤æ“ä½œé˜²æ­¢ã®ãŸã‚ã€ã‚¹ã‚³ã‚¢ãŒ 60 ä»¥ä¸Šã®æ™‚ã®ã¿è‡ªå‹•ä¿®å¾©ãƒœã‚¿ãƒ³ãŒæœ‰åŠ¹åŒ–ã•ã‚Œã¾ã™ã€‚
            """)

    # ãƒãƒ£ãƒƒãƒˆ (å¸¸æ™‚è¡¨ç¤º)
    with st.expander("ğŸ’¬ Chat with AI Agent", expanded=False):
        # å¯¾è±¡CIã®ã‚µãƒãƒªï¼ˆè¡¨ç¤ºã®ã¿ã€UXã¯å´©ã•ãšæœ€å°ï¼‰
        _chat_target_id = ""
        try:
            if selected_incident_candidate:
                _chat_target_id = selected_incident_candidate.get("id", "") or ""
        except Exception:
            _chat_target_id = ""
        if not _chat_target_id:
            _chat_target_id = target_device_id if 'target_device_id' in globals() else ""
        _chat_ci = _build_ci_context_for_chat(_chat_target_id) if _chat_target_id else {}
        if _chat_ci:
            _vendor = _chat_ci.get("vendor", "") or "Unknown"
            _os = _chat_ci.get("os", "") or "Unknown"
            _model = _chat_ci.get("model", "") or "Unknown"
            st.caption(f"å¯¾è±¡æ©Ÿå™¨: {_chat_target_id}   Vendor: {_vendor}   OS: {_os}   Model: {_model}")

        # ã‚¯ã‚¤ãƒƒã‚¯è³ªå•ï¼ˆå…¥åŠ›æ¬„ã¯å¤‰ãˆãšã€ã‚³ãƒ”ãƒšç”¨ã«æç¤ºï¼‰
        q1, q2, q3 = st.columns(3)
        if "chat_quick_text" not in st.session_state:
            st.session_state.chat_quick_text = ""

        with q1:
            if st.button("è¨­å®šãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—", use_container_width=True):
                st.session_state.chat_quick_text = "ã“ã®æ©Ÿå™¨ã§ã€ç¾åœ¨ã®è¨­å®šã‚’å®‰å…¨ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã™ã‚‹æ‰‹é †ã¨ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
        with q2:
            if st.button("ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯", use_container_width=True):
                st.session_state.chat_quick_text = "ã“ã®æ©Ÿå™¨ã§ã€å¤‰æ›´ã‚’ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã™ã‚‹ä»£è¡¨çš„ãªæ‰‹é †ï¼ˆå€™è£œï¼‰ã¨æ³¨æ„ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
        with q3:
            if st.button("ç¢ºèªã‚³ãƒãƒ³ãƒ‰", use_container_width=True):
                st.session_state.chat_quick_text = "ä»Šå›ã®ç—‡çŠ¶ã‚’åˆ‡ã‚Šåˆ†ã‘ã‚‹ãŸã‚ã«ã€ã¾ãšå®Ÿè¡Œã™ã¹ãç¢ºèªã‚³ãƒãƒ³ãƒ‰ï¼ˆshow/diagnosticï¼‰ã‚’å„ªå…ˆåº¦é †ã«æ•™ãˆã¦ãã ã•ã„ã€‚"

        if st.session_state.chat_quick_text:
            st.info("ã‚¯ã‚¤ãƒƒã‚¯è³ªå•ï¼ˆã‚³ãƒ”ãƒ¼ã—ã¦è²¼ã‚Šä»˜ã‘ï¼‰")
            st.code(st.session_state.chat_quick_text)

        if st.session_state.chat_session is None and api_key and selected_scenario != "æ­£å¸¸ç¨¼åƒ":
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel("gemma-3-12b-it")
            st.session_state.chat_session = model.start_chat(history=[])

        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])

        if prompt := st.chat_input("Ask details..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"): st.markdown(prompt)
            if st.session_state.chat_session:
                with st.chat_message("assistant"):
                    with st.spinner("Thinking..."):
                        res_container = st.empty()
                        # CI-aware promptï¼ˆCI/Config ã‚’ãƒ•ãƒ«æ´»ç”¨ï¼‰
                        target_id = ""
                        try:
                            if selected_incident_candidate:
                                target_id = selected_incident_candidate.get("id", "") or ""
                        except Exception:
                            target_id = ""
                        if not target_id:
                            try:
                                target_id = target_device_id
                            except Exception:
                                target_id = ""
                        ci = _build_ci_context_for_chat(target_id) if target_id else {}
                        ci_prompt = f"""ã‚ãªãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é‹ç”¨ï¼ˆNOC/SREï¼‰ã®å®Ÿå‹™è€…ã§ã™ã€‚
æ¬¡ã® CI æƒ…å ±ã¨ Config æŠœç²‹ã‚’å¿…ãšå‚ç…§ã—ã¦ã€å…·ä½“çš„ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚ä¸€èˆ¬è«–ã ã‘ã§çµ‚ã‚ã‚‰ã›ãªã„ã§ãã ã•ã„ã€‚

ã€CI (JSON)ã€‘
{json.dumps(ci, ensure_ascii=False, indent=2)}

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘
{prompt}

å›ç­”ãƒ«ãƒ¼ãƒ«:
- CI/Config ã«åŸºã¥ãå…·ä½“æ‰‹é †ãƒ»ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’æç¤ºã™ã‚‹
- è¿½åŠ ç¢ºèªãŒå¿…è¦ãªã‚‰ã€è³ªå•ã¯æœ€å°é™ï¼ˆ1ã€œ2ç‚¹ï¼‰ã«çµã‚‹
- ä¸æ˜ãªå‰æã¯æ¨æ¸¬ã›ãšã€ŒCIã«ç„¡ã„ã®ã§ç¢ºèªãŒå¿…è¦ã€ã¨æ˜è¨˜ã™ã‚‹
"""

                        response = generate_content_with_retry(st.session_state.chat_session.model, ci_prompt, stream=True)
                        if response:
                            full_response = ""
                            for chunk in response:
                                piece = _safe_chunk_text(chunk)
                                if not piece:
                                    continue
                                full_response += piece
                                res_container.markdown(full_response)
                            if not full_response.strip():
                                full_response = "AIå¿œç­”ãŒç©ºã§ã—ãŸï¼ˆCIã¯æ¸¡ã—ã¾ã—ãŸãŒå‡ºåŠ›ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼‰ã€‚"
                            st.session_state.messages.append({"role": "assistant", "content": full_response})
                        else:
                            st.error("AIã‹ã‚‰ã®å¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# ãƒ™ã‚¤ã‚ºæ›´æ–°ãƒˆãƒªã‚¬ãƒ¼ (è¨ºæ–­å¾Œ)
if st.session_state.trigger_analysis and st.session_state.live_result:
    if st.session_state.verification_result:
        pass
    st.session_state.trigger_analysis = False
    st.rerun()
