# -*- coding: utf-8 -*-
"""
Google Antigravity AIOps Agent - Streamlit Main Application
å®Œå…¨ç‰ˆ: ã‚¢ãƒ©ãƒ¼ãƒ é¸åˆ¥ã€çœŸå› ç‰¹å®šã€ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰éšœå®³åˆ†æ
"""

import streamlit as st
import os
import json
import time
from typing import List, Dict, Any
import google.generativeai as genai

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from data import TOPOLOGY, NetworkNode
from logic import CausalInferenceEngine, Alarm, simulate_cascade_failure
from inference_engine import LogicalRCA
from verifier import verify_log_content, format_verification_report
from network_ops import (
    generate_fake_log_by_ai,
    run_diagnostic_simulation,
    generate_remediation_commands,
    generate_health_check_commands
)

# =====================================================
# ãƒšãƒ¼ã‚¸è¨­å®š
# =====================================================
st.set_page_config(
    page_title="AIOps - éšœå®³åˆ†æã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ›¡ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =====================================================
# å®šæ•°å®šç¾©
# =====================================================
SCENARIOS = {
    "æ­£å¸¸ç¨¼åƒ": "æ­£å¸¸ç¨¼åƒ",
    "--- WANãƒ«ãƒ¼ã‚¿ãƒ¼éšœå®³ ---": None,
    "[WAN]é›»æºéšœå®³ï¼šç‰‡ç³»": "[WAN]é›»æºéšœå®³ï¼šç‰‡ç³»",
    "[WAN]é›»æºéšœå®³ï¼šä¸¡ç³»": "[WAN]é›»æºéšœå®³ï¼šä¸¡ç³»",
    "[WAN]BGPãƒ•ãƒ©ãƒƒãƒ”ãƒ³ã‚°": "[WAN]BGPãƒ•ãƒ©ãƒƒãƒ”ãƒ³ã‚°",
    "[WAN]ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯": "[WAN]ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯",
    "--- ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ« ---": None,
    "[FW]é›»æºéšœå®³ï¼šç‰‡ç³»": "[FW]é›»æºéšœå®³ï¼šç‰‡ç³»",
    "[FW]é›»æºéšœå®³ï¼šä¸¡ç³»": "[FW]é›»æºéšœå®³ï¼šä¸¡ç³»",
    "[FW]FANæ•…éšœ": "[FW]FANæ•…éšœ",
    "[FW]ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯": "[FW]ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯",
    "--- L2ã‚¹ã‚¤ãƒƒãƒ ---": None,
    "[L2SW]é›»æºéšœå®³ï¼šç‰‡ç³»": "[L2SW]é›»æºéšœå®³ï¼šç‰‡ç³»",
    "[L2SW]é›»æºéšœå®³ï¼šä¸¡ç³»": "[L2SW]é›»æºéšœå®³ï¼šä¸¡ç³»",
    "[L2SW]FANæ•…éšœ": "[L2SW]FANæ•…éšœ",
    "[L2SW]ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯": "[L2SW]ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯",
    "[L2SW]ã‚µã‚¤ãƒ¬ãƒ³ãƒˆéšœå®³": "[L2SW]ã‚µã‚¤ãƒ¬ãƒ³ãƒˆéšœå®³",
    "--- ã‚¢ã‚¯ã‚»ã‚¹ãƒã‚¤ãƒ³ãƒˆ ---": None,
    "[AP]AP_01ãƒ€ã‚¦ãƒ³": "[AP]AP_01ãƒ€ã‚¦ãƒ³",
    "[AP]AP_01ã‚±ãƒ¼ãƒ–ãƒ«éšœå®³": "[AP]AP_01ã‚±ãƒ¼ãƒ–ãƒ«éšœå®³",
    "--- å¤šé‡éšœå®³ ---": None,
    "[è¤‡åˆ]FW_01_PRIMARYã¨AP_03ã®å¤šé‡éšœå®³": "[è¤‡åˆ]FW_01_PRIMARYã¨AP_03ã®å¤šé‡éšœå®³",
    "[è¤‡åˆ]WANé›»æºç‰‡ç³»+FANå¤šé‡éšœå®³": "[è¤‡åˆ]WANé›»æºç‰‡ç³»+FANå¤šé‡éšœå®³",
}

# =====================================================
# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
# =====================================================
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False
if 'current_scenario' not in st.session_state:
    st.session_state.current_scenario = None
if 'root_cause_result' not in st.session_state:
    st.session_state.root_cause_result = None
if 'generated_log' not in st.session_state:
    st.session_state.generated_log = ""
if 'remediation_executed' not in st.session_state:
    st.session_state.remediation_executed = False
if 'health_check_done' not in st.session_state:
    st.session_state.health_check_done = False

# =====================================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# =====================================================

def get_target_node_from_scenario(scenario: str) -> str:
    """ã‚·ãƒŠãƒªã‚ªã‹ã‚‰å¯¾è±¡ãƒãƒ¼ãƒ‰IDã‚’æ¨å®š"""
    if "[WAN]" in scenario:
        return "WAN_ROUTER_01"
    elif "[FW]" in scenario:
        return "FW_01_PRIMARY"
    elif "[L2SW]" in scenario:
        return "L2_SW_01"
    elif "[AP]" in scenario:
        return "AP_01"
    elif "FW_01_PRIMARYã¨AP_03" in scenario:
        return "FW_01_PRIMARY"
    elif "WANé›»æº" in scenario:
        return "WAN_ROUTER_01"
    return "WAN_ROUTER_01"

def generate_massive_alarms(scenario: str, root_device_id: str) -> List[Alarm]:
    """
    å¤§é‡ã®å†—é•·ã‚¢ãƒ©ãƒ¼ãƒ ã‚’ç”Ÿæˆï¼ˆ50-200ä»¶ï¼‰
    å®Ÿéš›ã®é‹ç”¨ã§ã¯ã€é…ä¸‹ã®å…¨æ©Ÿå™¨ã‹ã‚‰æ§˜ã€…ãªã‚¢ãƒ©ãƒ¼ãƒ ãŒä¸ŠãŒã£ã¦ãã‚‹
    """
    import random
    
    alarms = []
    root_node = TOPOLOGY.get(root_device_id)
    
    if not root_node:
        return alarms
    
    # æ ¹æœ¬åŸå› ã®ã‚¢ãƒ©ãƒ¼ãƒ 
    if "é›»æº" in scenario:
        if "ä¸¡ç³»" in scenario:
            alarms.append(Alarm(root_device_id, "Power Supply 1 Failed", "CRITICAL"))
            alarms.append(Alarm(root_device_id, "Power Supply 2 Failed", "CRITICAL"))
            alarms.append(Alarm(root_device_id, "Device Unreachable", "CRITICAL"))
        else:
            alarms.append(Alarm(root_device_id, "Power Supply 1 Failed", "WARNING"))
            alarms.append(Alarm(root_device_id, "Redundancy Lost", "WARNING"))
    elif "BGP" in scenario:
        alarms.append(Alarm(root_device_id, "BGP Peer Flapping", "CRITICAL"))
        alarms.append(Alarm(root_device_id, "Route Instability Detected", "WARNING"))
    elif "FAN" in scenario:
        alarms.append(Alarm(root_device_id, "Fan Module Failed", "CRITICAL"))
        alarms.append(Alarm(root_device_id, "Temperature Warning", "WARNING"))
    elif "ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯" in scenario:
        alarms.append(Alarm(root_device_id, "Memory Usage 95%", "CRITICAL"))
        alarms.append(Alarm(root_device_id, "System Performance Degraded", "WARNING"))
    elif "ã‚±ãƒ¼ãƒ–ãƒ«" in scenario:
        alarms.append(Alarm(root_device_id, "Interface GigabitEthernet0/1 Down", "CRITICAL"))
        alarms.append(Alarm(root_device_id, "Link Status Changed", "WARNING"))
    elif "ãƒ€ã‚¦ãƒ³" in scenario:
        alarms.append(Alarm(root_device_id, "Device Down", "CRITICAL"))
        alarms.append(Alarm(root_device_id, "SNMP Timeout", "CRITICAL"))
    
    # ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰éšœå®³ã®ã‚¢ãƒ©ãƒ¼ãƒ ç”Ÿæˆ
    cascade_alarms = simulate_cascade_failure(root_device_id, TOPOLOGY, "Connection Lost")
    alarms.extend(cascade_alarms[1:])  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚æ ¹æœ¬åŸå› ä»¥å¤–ã‚’è¿½åŠ 
    
    # ãƒã‚¤ã‚ºã‚¢ãƒ©ãƒ¼ãƒ ã‚’å¤§é‡è¿½åŠ ï¼ˆ50-200ä»¶ã«ï¼‰
    noise_messages = [
        "SNMP Trap Received",
        "Interface Utilization 50%",
        "Minor Configuration Change",
        "Backup Job Started",
        "User Login Detected",
        "Temperature Normal",
        "Fan Speed Adjusted",
        "ARP Cache Updated",
        "Routing Table Updated",
        "VLAN Database Modified",
        "ACL Hit Count Threshold",
        "Port Security Violation (Info)",
        "NTP Sync OK",
        "DNS Query Timeout (Retry OK)",
        "DHCP Lease Expired (Auto Renewed)",
    ]
    
    target_count = random.randint(50, 200)
    while len(alarms) < target_count:
        random_device = random.choice(list(TOPOLOGY.keys()))
        random_message = random.choice(noise_messages)
        random_severity = random.choice(["INFO", "WARNING", "INFO", "INFO"])  # INFOå¤šã‚
        alarms.append(Alarm(random_device, random_message, random_severity))
    
    return alarms

def filter_critical_alarms(all_alarms: List[Alarm], api_key: str) -> List[Alarm]:
    """
    AIã‚’ä½¿ã£ã¦æœ¬å½“ã«é‡è¦ãªã‚¢ãƒ©ãƒ¼ãƒ ã ã‘ã‚’3-5ä»¶ã«çµã‚‹
    """
    if not api_key:
        # APIã‚­ãƒ¼ãŒãªã„å ´åˆã¯CRITICALã®ã¿è¿”ã™
        return [a for a in all_alarms if a.severity == "CRITICAL"][:5]
    
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-1.5-flash')
    
    # ã‚¢ãƒ©ãƒ¼ãƒ æƒ…å ±ã‚’æ•´å½¢
    alarm_list = "\n".join([
        f"{i+1}. Device: {a.device_id}, Message: {a.message}, Severity: {a.severity}"
        for i, a in enumerate(all_alarms[:100])  # æœ€åˆã®100ä»¶ã®ã¿é€ä¿¡
    ])
    
    prompt = f"""
ã‚ãªãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¢ãƒ©ãƒ¼ãƒ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°AIã§ã™ã€‚
ä»¥ä¸‹ã®å¤§é‡ã®ã‚¢ãƒ©ãƒ¼ãƒ ã‹ã‚‰ã€**æ ¹æœ¬åŸå› ã«é–¢é€£ã™ã‚‹é‡è¦ãªã‚¢ãƒ©ãƒ¼ãƒ ã ã‘ã‚’3ã€œ5ä»¶é¸æŠ**ã—ã¦ãã ã•ã„ã€‚

ã€ã‚¢ãƒ©ãƒ¼ãƒ ãƒªã‚¹ãƒˆã€‘
{alarm_list}

ã€é¸æŠãƒ«ãƒ¼ãƒ«ã€‘
1. CRITICAL / WARNING ã®é‡è¦ãªã‚¢ãƒ©ãƒ¼ãƒ ã‚’å„ªå…ˆ
2. INFOï¼ˆæƒ…å ±é€šçŸ¥ï¼‰ã¯åŸºæœ¬çš„ã«ç„¡è¦–
3. åŒã˜ãƒ‡ãƒã‚¤ã‚¹ã‹ã‚‰ã®é‡è¤‡ã‚¢ãƒ©ãƒ¼ãƒ ã¯1ã¤ã«ã¾ã¨ã‚ã‚‹
4. ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰éšœå®³ï¼ˆé…ä¸‹ã®æ©Ÿå™¨ã®Connection Lostï¼‰ã¯æ ¹æœ¬åŸå› ã§ã¯ãªã„ãŸã‚é™¤å¤–
5. é›»æºéšœå®³ã€Interface Downã€BGP Flappingã€Fan Failãªã©ã€Œç›´æ¥çš„ãªéšœå®³ã€ã‚’é¸ã¶

ã€å‡ºåŠ›å½¢å¼ã€‘
é¸æŠã—ãŸã‚¢ãƒ©ãƒ¼ãƒ ã®ç•ªå·ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
ä¾‹: 1,3,5,12,18

ç•ªå·ã®ã¿ã‚’å‡ºåŠ›ã—ã€èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚
"""
    
    try:
        response = model.generate_content(prompt)
        selected_indices = [int(x.strip()) - 1 for x in response.text.strip().split(',')]
        return [all_alarms[i] for i in selected_indices if i < len(all_alarms)]
    except Exception as e:
        st.warning(f"AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        return [a for a in all_alarms if a.severity in ["CRITICAL", "WARNING"]][:5]

def get_cascade_impact(root_device_id: str) -> Dict[str, Any]:
    """
    ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰éšœå®³ã®å½±éŸ¿ç¯„å›²ã‚’åˆ†æ
    """
    affected_nodes = []
    root_node = TOPOLOGY.get(root_device_id)
    
    if not root_node:
        return {"count": 0, "nodes": [], "reason": ""}
    
    # BFSã§é…ä¸‹ã®ãƒãƒ¼ãƒ‰ã‚’åˆ—æŒ™
    queue = [root_device_id]
    processed = {root_device_id}
    
    while queue:
        current_id = queue.pop(0)
        children = [n for n in TOPOLOGY.values() if n.parent_id == current_id]
        
        for child in children:
            if child.id not in processed:
                affected_nodes.append(child)
                queue.append(child.id)
                processed.add(child.id)
    
    # ç†ç”±æ–‡ã‚’ç”Ÿæˆ
    reason = f"""
**ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰éšœå®³ã®è©³ç´°åˆ†æ**

ã€ç›´æ¥åŸå› ã€‘
{root_device_id} ãŒå®Œå…¨ã«ãƒ€ã‚¦ãƒ³ã—ã¦ã„ã¾ã™ã€‚

ã€ãªãœé…ä¸‹ã®æ©Ÿå™¨ãŒç›£è¦–ä¸èƒ½ãªã®ã‹ã€‘
{root_device_id} ã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒˆãƒãƒ­ã‚¸ãƒ¼ã®Layer {root_node.layer}ã«ä½ç½®ã—ã€
ã™ã¹ã¦ã®é€šä¿¡ã®ä¸­ç¶™ç‚¹ã¨ãªã£ã¦ã„ã¾ã™ã€‚ã“ã®ãƒ‡ãƒã‚¤ã‚¹ãŒãƒ€ã‚¦ãƒ³ã™ã‚‹ã¨ã€
é…ä¸‹ã®å…¨æ©Ÿå™¨ã¸ã®é€šä¿¡çµŒè·¯ãŒé®æ–­ã•ã‚Œã‚‹ãŸã‚ã€ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰åˆ°é”ä¸èƒ½ã¨ãªã‚Šã¾ã™ã€‚

ã€å½±éŸ¿ã‚’å—ã‘ã¦ã„ã‚‹æ©Ÿå™¨ï¼ˆ{len(affected_nodes)}å°ï¼‰ã€‘
"""
    
    for node in sorted(affected_nodes, key=lambda n: n.layer):
        reason += f"\nâ”œ {node.id} (Layer {node.layer}, {node.type})"
    
    reason += """

âš ï¸ **é‡è¦ãªæ³¨æ„äº‹é …**
ã“ã‚Œã‚‰ã®é…ä¸‹ã®æ©Ÿå™¨è‡ªä½“ã«ã¯éšœå®³ã¯ç™ºç”Ÿã—ã¦ã„ã¾ã›ã‚“ã€‚
ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµŒè·¯ãŒé®æ–­ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€Œç›£è¦–ä¸èƒ½ã€çŠ¶æ…‹ã«ãªã£ã¦ã„ã‚‹ã ã‘ã§ã™ã€‚
{root_device_id} ã‚’å¾©æ—§ã™ã‚Œã°ã€ã“ã‚Œã‚‰ã®æ©Ÿå™¨ã¯è‡ªå‹•çš„ã«æ­£å¸¸çŠ¶æ…‹ã«æˆ»ã‚Šã¾ã™ã€‚
"""
    
    return {
        "count": len(affected_nodes),
        "nodes": affected_nodes,
        "reason": reason
    }

def generate_topology_graph(root_cause_id: str = None, cascade_nodes: List[str] = None) -> str:
    """
    Graphvizãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒˆãƒãƒ­ã‚¸ãƒ¼å›³ã‚’ç”Ÿæˆ
    è‰²åˆ†ã‘: èµ¤=çœŸå› ã€ã‚ªãƒ¬ãƒ³ã‚¸=ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å½±éŸ¿ã€ç·‘=æ­£å¸¸
    """
    cascade_set = set(cascade_nodes) if cascade_nodes else set()
    
    dot = """
digraph Topology {
    rankdir=TB;
    node [shape=box, style=filled];
    
"""
    
    for node_id, node in TOPOLOGY.items():
        if node_id == root_cause_id:
            color = "red"
            label = f"{node_id}\\nâŒ çœŸå› "
        elif node_id in cascade_set:
            color = "orange"
            label = f"{node_id}\\nâš ï¸ ç›£è¦–ä¸èƒ½"
        else:
            color = "lightgreen"
            label = node_id
        
        dot += f'    "{node_id}" [label="{label}", fillcolor={color}];\n'
    
    # ã‚¨ãƒƒã‚¸ã®è¿½åŠ 
    for node_id, node in TOPOLOGY.items():
        if node.parent_id:
            dot += f'    "{node.parent_id}" -> "{node_id}";\n'
    
    dot += "}\n"
    return dot

# =====================================================
# ãƒ¡ã‚¤ãƒ³ç”»é¢
# =====================================================

def main():
    st.title("ğŸ›¡ï¸ AIOps éšœå®³åˆ†æã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("---")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        
        # APIã‚­ãƒ¼è¨­å®š
        api_key = os.environ.get("GOOGLE_API_KEY", "")
        if not api_key:
            api_key = st.text_input("Google API Key", type="password")
            if api_key:
                os.environ["GOOGLE_API_KEY"] = api_key
        else:
            st.success("âœ… APIã‚­ãƒ¼è¨­å®šæ¸ˆã¿")
        
        st.markdown("---")
        
        # ã‚·ãƒŠãƒªã‚ªé¸æŠ
        st.subheader("ğŸ“‹ éšœå®³ã‚·ãƒŠãƒªã‚ªé¸æŠ")
        scenario_keys = [k for k in SCENARIOS.keys() if SCENARIOS[k] is not None]
        selected_scenario = st.selectbox(
            "ã‚·ãƒŠãƒªã‚ªã‚’é¸æŠ",
            scenario_keys,
            index=0
        )
        
        st.markdown("---")
        
        # åˆ†æå®Ÿè¡Œãƒœã‚¿ãƒ³
        if st.button("ğŸš€ éšœå®³åˆ†æã‚’å®Ÿè¡Œ", type="primary", use_container_width=True):
            if not api_key:
                st.error("âŒ APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
            else:
                st.session_state.current_scenario = selected_scenario
                st.session_state.analysis_done = False
                st.session_state.remediation_executed = False
                st.session_state.health_check_done = False
                st.rerun()
        
        # ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
        if st.button("ğŸ”„ ãƒªã‚»ãƒƒãƒˆ", use_container_width=True):
            st.session_state.analysis_done = False
            st.session_state.current_scenario = None
            st.session_state.root_cause_result = None
            st.session_state.generated_log = ""
            st.session_state.remediation_executed = False
            st.session_state.health_check_done = False
            st.rerun()
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    if st.session_state.current_scenario and not st.session_state.analysis_done:
        perform_analysis(st.session_state.current_scenario, api_key)
    
    if st.session_state.analysis_done and st.session_state.root_cause_result:
        display_results(st.session_state.root_cause_result, api_key)

def perform_analysis(scenario: str, api_key: str):
    """éšœå®³åˆ†æã‚’å®Ÿè¡Œ"""
    
    with st.spinner("ğŸ” éšœå®³åˆ†æã‚’å®Ÿè¡Œä¸­..."):
        # 1. å¯¾è±¡ãƒãƒ¼ãƒ‰ç‰¹å®š
        target_device_id = get_target_node_from_scenario(scenario)
        target_node = TOPOLOGY.get(target_device_id)
        
        if not target_node:
            st.error(f"âŒ ãƒ‡ãƒã‚¤ã‚¹ {target_device_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # 2. éšœå®³ãƒ­ã‚°ç”Ÿæˆ
        st.info("ğŸ“ ã‚¹ãƒ†ãƒƒãƒ—1: éšœå®³ãƒ­ã‚°ã‚’ç”Ÿæˆä¸­...")
        time.sleep(0.5)
        
        log_result = run_diagnostic_simulation(scenario, target_node, api_key)
        generated_log = log_result.get("sanitized_log", "")
        st.session_state.generated_log = generated_log
        
        # 3. å¤§é‡ã‚¢ãƒ©ãƒ¼ãƒ ç”Ÿæˆ
        st.info("ğŸš¨ ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¢ãƒ©ãƒ¼ãƒ ã‚’ç”Ÿæˆä¸­ï¼ˆ50-200ä»¶ï¼‰...")
        time.sleep(0.5)
        
        all_alarms = generate_massive_alarms(scenario, target_device_id)
        st.success(f"âœ… {len(all_alarms)}ä»¶ã®ã‚¢ãƒ©ãƒ¼ãƒ ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        
        # 4. AIã‚¢ãƒ©ãƒ¼ãƒ é¸åˆ¥
        st.info("ğŸ¯ ã‚¹ãƒ†ãƒƒãƒ—3: AIãŒé‡è¦ãªã‚¢ãƒ©ãƒ¼ãƒ ã‚’é¸åˆ¥ä¸­...")
        time.sleep(1.0)
        
        critical_alarms = filter_critical_alarms(all_alarms, api_key)
        st.success(f"âœ… {len(critical_alarms)}ä»¶ã®é‡è¦ã‚¢ãƒ©ãƒ¼ãƒ ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
        
        # 5. ãƒ­ã‚°æ¤œè¨¼
        st.info("ğŸ”¬ ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ­ã‚°ã‚’æ¤œè¨¼ä¸­...")
        time.sleep(0.5)
        
        verification = verify_log_content(generated_log)
        
        # 6. å› æœæ¨è«–
        st.info("ğŸ§  ã‚¹ãƒ†ãƒƒãƒ—5: å› æœæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã§çœŸå› ã‚’ç‰¹å®šä¸­...")
        time.sleep(1.0)
        
        engine = CausalInferenceEngine(TOPOLOGY)
        inference_result = engine.analyze_alarms(critical_alarms)
        
        # 7. LLMå†—é•·æ€§åˆ†æ
        st.info("ğŸ¤– ã‚¹ãƒ†ãƒƒãƒ—6: LLMã§å†—é•·æ€§ã‚’åˆ†æä¸­...")
        time.sleep(1.0)
        
        rca = LogicalRCA(TOPOLOGY)
        llm_analysis = rca.analyze(critical_alarms)
        
        # 8. ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å½±éŸ¿åˆ†æ
        st.info("ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—7: ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å½±éŸ¿ã‚’åˆ†æä¸­...")
        time.sleep(0.5)
        
        cascade_impact = get_cascade_impact(target_device_id)
        
        # 9. å¾©æ—§æ‰‹é †ç”Ÿæˆ
        st.info("ğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—8: å¾©æ—§æ‰‹é †ã‚’ç”Ÿæˆä¸­...")
        time.sleep(1.0)
        
        remediation = generate_remediation_commands(
            scenario,
            llm_analysis[0] if llm_analysis else {},
            target_node,
            api_key
        )
        
        # çµæœã‚’ä¿å­˜
        st.session_state.root_cause_result = {
            "scenario": scenario,
            "target_device": target_device_id,
            "target_node": target_node,
            "all_alarms_count": len(all_alarms),
            "critical_alarms": critical_alarms,
            "inference_result": inference_result,
            "llm_analysis": llm_analysis,
            "verification": verification,
            "cascade_impact": cascade_impact,
            "remediation": remediation,
            "generated_log": generated_log
        }
        
        st.session_state.analysis_done = True
        st.rerun()

def display_results(result: Dict[str, Any], api_key: str):
    """åˆ†æçµæœã‚’è¡¨ç¤º"""
    
    # 1. çœŸå› ç‰¹å®šã®è¡¨ç¤º
    st.markdown("## ğŸ¯ çœŸå› ç‰¹å®šçµæœ")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "ğŸ“‰ ãƒã‚¤ã‚ºå‰Šæ¸›ç‡",
            f"{((result['all_alarms_count'] - len(result['critical_alarms'])) / result['all_alarms_count'] * 100):.1f}%"
        )
    
    with col2:
        st.metric(
            "ğŸ“¨ ç·ã‚¢ãƒ©ãƒ¼ãƒ æ•°",
            f"{result['all_alarms_count']}ä»¶"
        )
    
    with col3:
        st.metric(
            "âœ… é¸åˆ¥å¾Œã‚¢ãƒ©ãƒ¼ãƒ ",
            f"{len(result['critical_alarms'])}ä»¶"
        )
    
    with col4:
        st.metric(
            "ğŸ¯ çœŸå› ",
            "1ä»¶ç‰¹å®š"
        )
    
    st.markdown("---")
    
    # çœŸå› ã®å¤§ããªè¡¨ç¤º
    inference = result['inference_result']
    root_node = inference.root_cause_node
    
    if root_node:
        st.markdown(f"""
### ğŸš¨ çœŸå› ç‰¹å®šå®Œäº†

<div style="background-color: #ff4444; padding: 20px; border-radius: 10px; color: white;">
<h2 style="color: white;">ãƒ‡ãƒã‚¤ã‚¹: {root_node.id}</h2>
<h3 style="color: white;">éšœå®³ç¨®åˆ¥: {result['scenario']}</h3>
<p style="font-size: 18px;"><strong>å½±éŸ¿åº¦:</strong> {inference.severity}</p>
<p style="font-size: 18px;"><strong>ç¢ºä¿¡åº¦:</strong> {result['llm_analysis'][0]['prob'] * 100:.0f}%</p>
<p style="font-size: 16px;"><strong>ç†ç”±:</strong> {inference.root_cause_reason}</p>
</div>
""", unsafe_allow_html=True)
    
    st.markdown("---")
    
    # 2. ãƒãƒ§ã‚¤ã‚¹ã•ã‚ŒãŸã‚¢ãƒ©ãƒ¼ãƒ è¡¨ç¤º
    with st.expander("ğŸš¨ ãƒãƒ§ã‚¤ã‚¹ã•ã‚ŒãŸé‡è¦ã‚¢ãƒ©ãƒ¼ãƒ ", expanded=True):
        for i, alarm in enumerate(result['critical_alarms'], 1):
            severity_emoji = "ğŸ”´" if alarm.severity == "CRITICAL" else "ğŸŸ¡" if alarm.severity == "WARNING" else "âšª"
            st.markdown(f"{severity_emoji} **{i}.** `{alarm.device_id}` - {alarm.message} ({alarm.severity})")
    
    st.markdown("---")
    
    # 3. ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å½±éŸ¿ã®èª¬æ˜
    cascade = result['cascade_impact']
    if cascade['count'] > 0:
        with st.expander("ğŸ“Š ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰éšœå®³ã®å½±éŸ¿åˆ†æ", expanded=True):
            st.markdown(cascade['reason'])
    
    st.markdown("---")
    
    # 4. ãƒˆãƒãƒ­ã‚¸ãƒ¼å›³
    st.markdown("## ğŸ—ºï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒˆãƒãƒ­ã‚¸ãƒ¼ï¼ˆè‰²åˆ†ã‘è¡¨ç¤ºï¼‰")
    
    cascade_node_ids = [n.id for n in cascade['nodes']]
    topology_graph = generate_topology_graph(
        root_cause_id=result['target_device'],
        cascade_nodes=cascade_node_ids
    )
    
    st.graphviz_chart(topology_graph)
    
    st.markdown("""
**å‡¡ä¾‹:**
- ğŸ”´ èµ¤: çœŸå› ï¼ˆæ ¹æœ¬åŸå› ï¼‰
- ğŸŸ  ã‚ªãƒ¬ãƒ³ã‚¸: ç›£è¦–ä¸èƒ½ï¼ˆã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å½±éŸ¿ï¼‰
- ğŸŸ¢ ç·‘: æ­£å¸¸ç¨¼åƒ
""")
    
    st.markdown("---")
    
    # 5. æ ¹æœ¬åŸå› åˆ†æçµæœ
    with st.expander("ğŸ” æ ¹æœ¬åŸå› åˆ†æã®è©³ç´°", expanded=True):
        st.markdown(f"""
**æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³åˆ†æ:**
- SOP Key: `{inference.sop_key}`
- é–¢é€£ã‚¢ãƒ©ãƒ¼ãƒ æ•°: {len(inference.related_alarms)}ä»¶

**LLMåˆ†æçµæœ:**
""")
        for analysis in result['llm_analysis']:
            st.json(analysis)
        
        st.markdown("**ãƒ­ã‚°æ¤œè¨¼çµæœï¼ˆGround Truthï¼‰:**")
        st.text(format_verification_report(result['verification']))
    
    st.markdown("---")
    
    # 6. å¾©æ—§æ‰‹é †
    st.markdown("## ğŸ“‹ å¾©æ—§æ‰‹é †")
    
    st.markdown(result['remediation'])
    
    st.markdown("---")
    
    # 7. å¾©æ—§æªç½®ãƒœã‚¿ãƒ³
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ”§ å¾©æ—§æªç½®ã‚’å®Ÿè¡Œ", type="primary", use_container_width=True):
            with st.spinner("å¾©æ—§æªç½®ã‚’å®Ÿè¡Œä¸­..."):
                time.sleep(2)
                st.session_state.remediation_executed = True
                st.rerun()
    
    with col2:
        if st.button("âœ… æ­£å¸¸æ€§ç¢ºèª", use_container_width=True):
            with st.spinner("æ­£å¸¸æ€§ç¢ºèªä¸­..."):
                time.sleep(2)
                st.session_state.health_check_done = True
                st.rerun()
    
    # å¾©æ—§æªç½®ã®çµæœ
    if st.session_state.remediation_executed:
        st.success("âœ… å¾©æ—§æªç½®ãŒå®Œäº†ã—ã¾ã—ãŸ")
        st.info("""
**å®Ÿè¡Œå†…å®¹:**
- é›»æºãƒ¦ãƒ‹ãƒƒãƒˆã‚’äº¤æ›ã—ã¾ã—ãŸ
- ãƒ‡ãƒã‚¤ã‚¹ã‚’å†èµ·å‹•ã—ã¾ã—ãŸ
- ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®çŠ¶æ…‹ã‚’ç¢ºèªã—ã¾ã—ãŸ
""")
    
    # æ­£å¸¸æ€§ç¢ºèªã®çµæœ
    if st.session_state.health_check_done:
        if result['scenario'] == "æ­£å¸¸ç¨¼åƒ":
            st.success("âœ… ã™ã¹ã¦ã®ãƒ‡ãƒã‚¤ã‚¹ãŒæ­£å¸¸ã«ç¨¼åƒã—ã¦ã„ã¾ã™")
        else:
            # æ­£å¸¸æ€§ç¢ºèªã‚³ãƒãƒ³ãƒ‰ã‚’ç”Ÿæˆãƒ»å®Ÿè¡Œ
            target_node = result['target_node']
            health_commands = generate_health_check_commands(target_node, api_key)
            
            st.success("âœ… æ­£å¸¸æ€§ç¢ºèªãŒå®Œäº†ã—ã¾ã—ãŸ")
            st.markdown(f"""
**ç¢ºèªçµæœ:**
- ãƒ‡ãƒã‚¤ã‚¹ {result['target_device']} ã¯æ­£å¸¸ã«å¾©æ—§ã—ã¾ã—ãŸ
- ã™ã¹ã¦ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒ UP çŠ¶æ…‹ã§ã™
- é…ä¸‹ã®æ©Ÿå™¨ã‚‚æ­£å¸¸ã«é€šä¿¡å¯èƒ½ã§ã™

**å®Ÿè¡Œã—ãŸã‚³ãƒãƒ³ãƒ‰:**
{health_commands}
""")
    
    st.markdown("---")
    
    # 8. AIãƒãƒ£ãƒƒãƒˆæ¬„
    st.markdown("## ğŸ’¬ AIãƒãƒ£ãƒƒãƒˆï¼ˆè©³ç´°ç¢ºèªï¼‰")
    
    user_question = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", placeholder="ä¾‹: ã“ã®éšœå®³ã®å½±éŸ¿ç¯„å›²ã‚’æ•™ãˆã¦")
    
    if user_question:
        with st.spinner("AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­..."):
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-1.5-flash')
            
            context = f"""
ã‚ãªãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯éšœå®³åˆ†æã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆAIã§ã™ã€‚
ä»¥ä¸‹ã®æƒ…å ±ã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã€éšœå®³ã‚·ãƒŠãƒªã‚ªã€‘
{result['scenario']}

ã€çœŸå› ãƒ‡ãƒã‚¤ã‚¹ã€‘
{result['target_device']}

ã€åˆ†æçµæœã€‘
{inference.root_cause_reason}

ã€ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å½±éŸ¿ã€‘
{cascade['count']}å°ã®æ©Ÿå™¨ãŒå½±éŸ¿ã‚’å—ã‘ã¦ã„ã¾ã™

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘
{user_question}
"""
            
            response = model.generate_content(context)
            st.markdown(f"**AIå›ç­”:**\n\n{response.text}")

# =====================================================
# å®Ÿè¡Œ
# =====================================================
if __name__ == "__main__":
    main()
